\chapter{Conclusion}

In this thesis we have shown that a multi-kernel OS is practical for intra-machine replication in terms of performance. We have presented the thread-interleaving challenges in replicating concurrent applications and come up with two different replication modes, Deterministic Execution and Schedule Replication, which are both inspired by inter-machine replication techniques, to provide the consistent state across all the replicas. Both of them have achieved the same goal with two totally different approaches. Our evaluation discovered the performance characteristics and found that with the low messaging overhead in Popcorn Linux, despite the large amount of messages that is required by Schedule Replication, the overhead from the imbalanced logical time in Deterministic Execution can still be greater than the cost of sending synchronization messages. As a result in the benchmark we showed that Schedule Replication is always better than Deterministic Execution.

\section{Contributions}
This thesis presents the following contributions:

% answer the questions here blah...
\begin{itemize}
\item \textbf{To explore the performance characteristics of different replication modes on a intra-machine replication setup, we implemented two replication modes to synchronize the thread-interleavings for concurrent applications on Popcorn Linux.} Both of them achieved the same goal in two different directions: Deterministic Execution uses a deterministic algorithm to decide the order of execution on both primary and secondary; while Schedule Replication enforces the secondary replica to follow the non-deterministic execution order that happened previously on the primary kernel. 

\item \textbf{In order to transparently replicate the applications, we have implemented a set of runtime support to eliminate the non-determinism inside pthread library.} By wrapping a code section with \detstart\ and \detend\ system calls, the execution order of wrapped sections can be the same on both primary and secondary kernel. We implemented an instrumented pthread library that can be dynamically linked with LD\_PRELOAD technique, which can minimize the effort of modifying the applications code. \textbf{For our evaluation, we did no modification to pbzip2, added 1 line to mongoose and added around 60 lines to nginx}.

\item \textbf{We evaluated both replication modes with different concurrent applications, and discovered that Schedule Replication is better for our multi-kernel based replication.} For a computational, non-network application we had \textbf{14.27\% to 63.39\%} slowdown for Deterministic Execution and maximum \textbf{0.89\% to 36.3\%} slowdown for Schedule Replication. For two web servers we had maximum \textbf{1.6\% to 25.22\%} slowdown for Deterministic Execution and maximum \textbf{0.23\% to 1.96\%} slowdown for Schedule Replication. With the low-overhead messaging layer in Popcorn Linux, despite the large amount of messages that is needed by Schedule Replication, the overhead from imbalanced logical time in Deterministic Execution seems much higher. Without the presence of precise logic time balancing for Deterministic Execution, Schedule Replication is always the better choice.

\end{itemize}
\section{Future Work}
\subsection{Precise Tick Bump for Deterministic Execution}
The major overhead in Deterministic Execution is the imbalanced logical time. The more precise the logical time incremental is the less waiting time we spend on \detstart\ . In chapter ~\ref{chap:detexec} we described our solution with a profiling approach, but from the evaluation we see this is still not precise enough to have the best performance. While performance counters are not deterministic enough to do the job~\cite{weaver2008can}, Intel PT~\cite{intelpt} seems to be a promising approach to track the progress of the execution. PT is able to precisely track the actual behaviour of the execution (e.g. branch decisions) not just a counter of software events, as a result for the same executable, same input and same thread-interleaving, PT should always generate the same execution trace and be deterministic. With this dynamic tracing technique, we might be able to provide precise online tick bumping without having to instrument the code.

% describe PT in detail
\subsection{Per-Lock Synchronization}
From the benchmark results, we occurred more overhead when the type of locks increases. This is because with serializing all the lock acquisitions, we are breaking the parallelism of accessing of different locks. If we only synchronize the access order of each particular lock while relaxing the total order of all the other locks, we might be able to achieve higher parallelism.

We could give an unique ID to \detstart\ as an additional parameter to identify different deterministic sections. However, the reason we couldn't implement this is that there is no perfect solution to generate this ID. A naive solution is to manually instrument the code with \detstart\ and \detend\, and manually assign an ID to each deterministic section. This requires massive changes to existing code. Moreover, some applications require external libraries that includes lock primitives which makes this even harder.

On possible way is to use static analysis to identify the lock acquistions in the application. Midas~\cite{slember2006static} and CoreDet~\cite{bergan2010coredet} both use compiler techniques to automatically instrument lock acquisitions. However with our experience on multiple modern applications' source code, there are applications tend to have their own wrapper of pthread lock functions, which makes it hard to identify different locks.

For pthread primitives, given the fact that most of them have futex involved, we could use the futex address~\cite{drepper2005futexes}~\cite{franke2002fuss} as the unique IDs for each deterministic section. However during our test we found the primary and secondary replica cannot always have the same address for the same lock. This is expected because they are on separate Linux kernels and have different address spaces. Some deterministic systems address the problem of non-deterministic memory address allocation and mitigate the problem with special memory allocators ~\cite{bergan2010deterministic}~\cite{liu2011dthreads}, in the future if we are able to synchronize the address space on the replicas, we might be able to directly use futex addresses as IDs for deterministic sections and achieve transparent per-lock synchronization. 

\subsection{Arbitrary Number of Replicas}
%cite consensus here, mention paxos made transparent, rex
%Address how increasing replicas can impact speed for schedule replication
Current implementation only supports one primary replica and one secondary replica. However Popcorn Linux supports booting arbitrary number of kernels as long as the hardware has enough resources. It would be interesting to explore the possibility of having more than one secondary replica. The major challenge is to decide the communication model for multiple replicas.

One solution is to do "Chain-Messaging". In this model we chain all the replicas one by one, the primary sends replication messages to the 2nd replica, and 2nd replica sends messages to 3rd replica and so on. Since point to point messaging is sure strictly FIFO on Popcorn Linux, this approach makes sure that all the replicas will see the same sequence of replication messages. Another advantage is that this can minimize the latency for the primary replica since it only communicates with one kernel. However in this approach the performance of the nth replica will be held back by all previous n-1th replicas, it cannot proceed until all its previous replicas have committed their operations.

We can also do broadcasting from the primary replica. Since we don't have the guarantee that message broadcasting in Popcorn Linux can behave in strictly FIFO, we might need to introduce consensus protocol such as Paxos ~\cite{lamport2001paxos} to make sure that all the replicas see the requests in the same order.

Another challenge is that when multiple replicas involved, who is going to be the primary when the previous primary fails. This can also be solved by utilizing the leader selection mechanism of Paxos ~\cite{lamport2001paxos}.

\subsection{Hybrid Replication}
%same, cite consensus here, mention paxos made transparent, rex

It is also interesting to extend this work to inter-machine and intra-machine hybrid replica. We can use intra-machine replica to deal with processor and memory faults with fast recovery, and inter-machine backup to deal with critical power failure with slower recovery. This will also have the same consensus problems as we mentioned in the previous section.

In our intra-machine implementation, we benefit a lot from our low latency messaging layer, the average time for sending a message is below 5 microseconds. However in a inter-machine setup, we might see different result than what we showed in our evaluation. The higher cost of inter-machine communication might cause greater slow down in Schedule Replication. While for Deterministic Execution which usually needs less messages, it might perform better than Schedule Replication in this case.